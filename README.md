# Autonomous Multi-Agent Software Development Agency

A self-hosted AI-native development framework optimized for voice-first interaction and local LLM inference. Built for a hybrid infrastructure of Ubuntu servers and Windows workstations.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WINDOWS WORKSTATION (192.168.86.38)              â”‚
â”‚                    Primary Development + Fast Inference             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Google Antigravity IDE / Trae.ai IDE                            â”‚
â”‚  â€¢ llama.cpp + Vulkan (RX 550 4GB) for fast small-model inference  â”‚
â”‚  â€¢ Voice input capture (microphone)                                 â”‚
â”‚  â€¢ MCP servers bridging to infrastructure                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ API calls over LAN
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI SERVER (192.168.86.56)                        â”‚
â”‚                    Heavy Lifting + State Management                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Ollama (CPU) - larger models (7B-13B) for complex reasoning     â”‚
â”‚  â€¢ Faster-Whisper STT service                                       â”‚
â”‚  â€¢ Kokoro TTS service                                               â”‚
â”‚  â€¢ PostgreSQL - chat history, agent state                          â”‚
â”‚  â€¢ ChromaDB - vector memory                                         â”‚
â”‚  â€¢ Docker orchestration                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ Message queue
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EAST SERVER (192.168.86.51)                      â”‚
â”‚                    Supporting Services                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Redis - inter-agent message queue                               â”‚
â”‚  â€¢ n8n - workflow automation                                        â”‚
â”‚  â€¢ Nginx reverse proxy                                              â”‚
â”‚  â€¢ Monitoring (Netdata)                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Infrastructure Summary

| Server | Hostname | IP | Role | CPU | RAM | GPU |
|--------|----------|-----|------|-----|-----|-----|
| **Primary AI** | `ai` | 192.168.86.56 | LLM + Orchestration | Xeon E5-1650 v3 (6C/12T) | 32 GB | Display only |
| **Windows Dev** | `DESKTOP-LIBOQV5` | 192.168.86.38 | Development + Fast Inference | i5-8400 (6C/6T) | 16 GB | **RX 550 4GB** |
| **East Server** | `linux-home` | 192.168.86.51 | Supporting Services | i5-7500 (4C/4T) | 16 GB | Display only |

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose on AI server
- Ollama installed on AI server
- llama.cpp with Vulkan support on Windows (optional, for fast inference)
- Tailscale for secure mesh networking (recommended)

### 1. Clone and Configure

```bash
git clone https://github.com/yourusername/autonomous-dev-agency.git
cd autonomous-dev-agency

# Copy environment template
cp .env.example .env

# Edit with your settings
nano .env
```

### 2. Deploy AI Server Stack

```bash
# SSH to AI server
ssh ai@192.168.86.56

# Deploy core services
cd autonomous-dev-agency/deploy
./setup-ai-server.sh
```

### 3. Deploy Supporting Services (East Server)

```bash
# SSH to East server
ssh linux-home@192.168.86.51

# Deploy Redis and monitoring
cd autonomous-dev-agency/deploy
./setup-east-server.sh
```

### 4. Configure Windows Workstation

See [docs/windows-setup.md](docs/windows-setup.md) for llama.cpp and MCP server configuration.

## ğŸ“ Repository Structure

```
autonomous-dev-agency/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                    # CI/CD pipeline
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ commands/                     # Reusable Claude Code commands
â”‚   â”‚   â”œâ”€â”€ generate-prp.md
â”‚   â”‚   â”œâ”€â”€ execute-prp.md
â”‚   â”‚   â”œâ”€â”€ review-and-evolve.md
â”‚   â”‚   â””â”€â”€ security-audit.md
â”‚   â”œâ”€â”€ agents/                       # Specialized agent definitions
â”‚   â”‚   â”œâ”€â”€ backend-developer.md
â”‚   â”‚   â”œâ”€â”€ frontend-developer.md
â”‚   â”‚   â”œâ”€â”€ test-specialist.md
â”‚   â”‚   â””â”€â”€ security-reviewer.md
â”‚   â””â”€â”€ settings.local.json
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ docker-compose.yml            # Main stack for AI server
â”‚   â”œâ”€â”€ docker-compose.east.yml       # East server services
â”‚   â”œâ”€â”€ setup-ai-server.sh
â”‚   â”œâ”€â”€ setup-east-server.sh
â”‚   â””â”€â”€ setup-windows.ps1
â”œâ”€â”€ voice-assistant/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.py                        # Gradio-based voice UI
â”‚   â”œâ”€â”€ stt_client.py
â”‚   â”œâ”€â”€ tts_client.py
â”‚   â”œâ”€â”€ llm_router.py
â”‚   â””â”€â”€ memory_manager.py
â”œâ”€â”€ mcp-servers/
â”‚   â”œâ”€â”€ ollama-mcp/                   # MCP server for Ollama
â”‚   â”œâ”€â”€ voice-mcp/                    # MCP server for voice pipeline
â”‚   â””â”€â”€ memory-mcp/                   # MCP server for agent memory
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ agents.yaml
â”‚   â”‚   â”œâ”€â”€ tasks.yaml
â”‚   â”‚   â””â”€â”€ guardrails.yaml
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ planning_agent.py
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ claude_code_tool.py
â”‚       â””â”€â”€ escalation_tool.py
â”œâ”€â”€ PRPs/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ prp_base.md
â”‚   â””â”€â”€ active/
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ episodic/
â”‚   â””â”€â”€ semantic/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ infrastructure-plan.md
â”‚   â”œâ”€â”€ windows-setup.md
â”‚   â”œâ”€â”€ voice-pipeline.md
â”‚   â””â”€â”€ mcp-integration.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ health-check.sh
â”‚   â”œâ”€â”€ backup-memory.sh
â”‚   â””â”€â”€ model-management.sh
â”œâ”€â”€ .env.example
â”œâ”€â”€ CLAUDE.md                         # Global Claude Code rules
â”œâ”€â”€ AGENTS.md                         # Root agent instructions
â””â”€â”€ README.md
```

## ğŸ¤ Voice Pipeline

The voice assistant provides natural voice interaction with your development environment:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Microphone   â”‚â”€â”€â”€â–ºâ”‚ Faster-      â”‚â”€â”€â”€â–ºâ”‚ LLM Router   â”‚â”€â”€â”€â–ºâ”‚ Kokoro TTS   â”‚
â”‚ (Windows)    â”‚    â”‚ Whisper      â”‚    â”‚              â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                     â”‚                    â”‚
                         â–¼                     â–¼                    â–¼
                    ~300-500ms          Quick: ~200ms         ~500ms-1s
                                       Complex: ~3-8s
```

### Latency Targets

| Response Type | Target | Path |
|--------------|--------|------|
| Quick acknowledgment | <1.5s | Windows GPU (Qwen 1.5B) |
| Simple query | <3s | AI Server CPU (Qwen 3B) |
| Complex reasoning | <10s | AI Server CPU (Qwen 7B) or Claude API |

## ğŸ¤– Agent Roles

The framework defines specialized agent roles that can be combined or split based on team size:

| Role | Primary Function | Tool Access |
|------|------------------|-------------|
| **Orchestrator** | Task coordination | Subagent management |
| **Senior Developer** | Complex implementation | Full filesystem, git |
| **Test Specialist** | Test strategy, automation | Test frameworks |
| **Security Reviewer** | Vulnerability analysis | SAST tools, read-only |
| **Documentation Writer** | Technical docs | Markdown tools |

See [docs/architecture.md](docs/architecture.md) for full role definitions.

## ğŸ“Š Monitoring

Health checks and metrics are available at:

- **Netdata**: http://192.168.86.51:19999 (East server)
- **Ollama Status**: http://192.168.86.56:11434/api/tags
- **Voice Pipeline Health**: http://192.168.86.56:7860/health

## ğŸ”§ Configuration

### Environment Variables

```bash
# AI Server
OLLAMA_HOST=http://localhost:11434
POSTGRES_PASSWORD=your-secure-password
CHROMA_HOST=localhost
CHROMA_PORT=8001

# Voice Pipeline
STT_URL=http://localhost:8000
TTS_URL=http://localhost:8880
WHISPER_MODEL=tiny.en

# Claude API (optional, for complex tasks)
ANTHROPIC_API_KEY=your-api-key
```

### Model Configuration

| Component | Model | Location | VRAM/RAM |
|-----------|-------|----------|----------|
| Fast inference | qwen2.5-1.5b-q4_k_m | Windows (llama.cpp) | 1.5 GB VRAM |
| STT | faster-whisper-tiny | AI Server | 2 GB RAM |
| Complex LLM | qwen2.5:7b | AI Server (Ollama) | 6 GB RAM |
| TTS | kokoro-82m | AI Server | 2 GB RAM |
| Embeddings | nomic-embed-text | AI Server (Ollama) | 500 MB RAM |

## ğŸ“ˆ Scaling Path

| Phase | Timeline | Changes |
|-------|----------|---------|
| **Current** | Now | CPU-only inference, voice assistant operational |
| **Phase 2** | Month 1-2 | Add GPU to AI server (RTX 3060 12GB recommended) |
| **Phase 3** | Month 3+ | Dedicated inference server or cloud hybrid |

## ğŸ›¡ï¸ Guardrails

The framework implements four-layer guardrails to prevent runaway agent behavior:

1. **Input Guardrails**: Task boundary validation, injection detection
2. **Action Guardrails**: RBAC, rate limits, autonomy thresholds
3. **Output Guardrails**: PII scanning, confidence scoring
4. **Operational Guardrails**: Circuit breakers, token budgets, time-boxing

## ğŸ“š Documentation

- [Architecture Deep Dive](docs/architecture.md)
- [Infrastructure Plan](docs/infrastructure-plan.md)
- [Windows Setup Guide](docs/windows-setup.md)
- [Voice Pipeline Details](docs/voice-pipeline.md)
- [MCP Integration](docs/mcp-integration.md)

## ğŸ¤ Contributing

This is a personal project, but suggestions and improvements are welcome via issues.

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.
